{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Crentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "BEARER_TOKEN = config['BEARER_TOKEN_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='App_LikedTweets_ServerStatus.log', encoding='utf-8', level=logging.ERROR, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liked Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def main(url, params):\n",
    "    json_response = connect_to_endpoint(url, params)\n",
    "    # print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dict_object(object_dict: dict, column_prefix: str,key_names_list: list = []) -> dict:\n",
    "    \"\"\"Return key,value pairs from dict. Returns selected keys in key_names_list or all if key_names_list is []\n",
    "\n",
    "    Args:\n",
    "        object_dict (dict)\n",
    "        key_names_list (list)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    result_dict = defaultdict(list)\n",
    "    for key_name, value_name in object_dict.items():\n",
    "        column_name = f\"{column_prefix}_{key_name}\"\n",
    "        if not key_names_list:\n",
    "            result_dict[column_name] = value_name\n",
    "        elif key_name in key_names_list:\n",
    "            result_dict[column_name] = value_name\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_items(list_of_dict: list, key_name: str) -> list:\n",
    "    if not list_of_dict: # If list_of_dict is None\n",
    "        return []\n",
    "    return [entity[key_name] for entity in list_of_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities(tweet_entities: dict, object_dict: dict, prefix: str = '') -> dict:\n",
    "    entities_dict = defaultdict(list)\n",
    "\n",
    "    # Retrieve Entities Objects\n",
    "    for object_name, key_name in object_dict.items():\n",
    "        column_name = f\"{prefix}{object_name}_list\"\n",
    "        # print(object_name)\n",
    "        entities_dict[column_name] = get_list_of_items(tweet_entities.get(object_name), key_name)\n",
    "    return entities_dict\n",
    "    # entities_dict['hashtags_list'] = get_list_of_items(tweet_entities['hashtags'], 'tag')\n",
    "    # entities_dict['urls_list'] = get_list_of_items(tweet_entities['urls'], 'expanded_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_sample  = pd.read_csv(\"data/unique_1500users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users Ids\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {\n",
    "}\n",
    "# query_params = {}\n",
    "search_url = \"https://api.twitter.com/2/users/by/username/CarlsbadDodger\"\n",
    "# search_url=\"https://api.twitter.com/2/users/by/username/TeamYouTube\"\n",
    "# Snowstorm = 3001806479\n",
    "# Team Youtube\n",
    "output_tweets = main(search_url, query_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11          davidgerard\n",
       "12          hu_YUZHNOYE\n",
       "13           tmbrown327\n",
       "14      nekoyasumi_club\n",
       "15           jinjinhnka\n",
       "             ...       \n",
       "1495        cloudboyebk\n",
       "1496    KhogendraRupini\n",
       "1497    LigiaMauraCosta\n",
       "1498       EdgarAguirre\n",
       "1499         Mark738eis\n",
       "Name: screen_name, Length: 1489, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usernames_sample['screen_name'][11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CarronMrs\n",
    "\n",
    "for idx, name in enumerate(usernames_sample['screen_name']):\n",
    "    if name == 'EdgarAguirre':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = config['BEARER_TOKEN_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(set(list_of_ids), columns=['user_id']).to_csv('data/ids_1500.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EdgarAguirre\n",
      "Mark738eis\n"
     ]
    }
   ],
   "source": [
    "for user_name in usernames_sample['screen_name'][1498:]:\n",
    "    print(user_name)\n",
    "    # break\n",
    "    search_url = f\"https://api.twitter.com/2/users/by/username/{user_name}\"\n",
    "    # search_url=\"https://api.twitter.com/2/users/by/username/TeamYouTube\"\n",
    "    # Snowstorm = 3001806479\n",
    "    # Team Youtube\n",
    "    output_tweets = main(search_url, query_params)\n",
    "    try:\n",
    "        list_of_ids.append(output_tweets['data']['id'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'errors': [{'value': 'realkellerwade',\n",
       "   'detail': 'Could not find user with username: [realkellerwade].',\n",
       "   'title': 'Not Found Error',\n",
       "   'resource_type': 'user',\n",
       "   'parameter': 'username',\n",
       "   'resource_id': 'realkellerwade',\n",
       "   'type': 'https://api.twitter.com/2/problems/resource-not-found'}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {\n",
    "    'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "    'tweet.fields': 'created_at,public_metrics,entities,lang,possibly_sensitive,reply_settings,source,in_reply_to_user_id,geo,referenced_tweets',\n",
    "    'expansions': 'author_id',\n",
    "}\n",
    "# query_params = {}\n",
    "search_url = \"https://api.twitter.com/2/users/3031071234/liked_tweets\"\n",
    "# search_url=\"https://api.twitter.com/2/users/by/username/TeamYouTube\"\n",
    "# Snowstorm = 3001806479\n",
    "# Team Youtube\n",
    "output_tweets = main(search_url, query_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Liked Tweets from users AVAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './avax_data'\n",
    "dir_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/7czjs3z54yd54jz8j2ktmygc0000gn/T/ipykernel_21245/2770470333.py:1: DtypeWarning: Columns (6,7,8,9,10,11,12,13,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'{path}/{dir_list[0]}')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{path}/{dir_list[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_id_list = set(df['userid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tweets_columns = [\n",
    "        'id', 'author_id', 'possibly_sensitive', 'edit_history_tweet_ids', 'lang',\n",
    "        'source', 'reply_settings', 'text', 'created_at'\n",
    "    ]\n",
    "\n",
    "tweet_columns = [\n",
    "    'possibly_sensitive', 'text', 'source', 'id', 'created_at', 'lang', 'reply_settings', 'author_id', 'edit_history_tweet_ids', \n",
    "    'hashtags_list', 'urls_list', 'public_metrics_retweet_count', 'public_metrics_reply_count', 'public_metrics_like_count', \n",
    "    'public_metrics_quote_count', 'pagination_token', 'current_time'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = 300 # in seconds\n",
    "N = 5 # Number of retries. Maxium retry time = N*sleep (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now()\n",
    "\n",
    "date_str = f'{current_time.year}-{current_time.month:02d}-{current_time.day:02d}'\n",
    "time_str = f'{current_time.hour:02d}-{current_time.minute:02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4539ky98xkteilwrcj3etufe2ckx'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4803qh0y60v44vlfxy97j0k2addk'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4803k2jgno90eq1scih76won6tdf'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4803dp9cefr7ofnms8qi5sbzzbjt'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4544d7d2hp0y1dfuqowfjccvyfjn'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4544b1fjuh77xwxj6gf0scrubo6t'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw45444nuoprplk3bfc8ijzy2noojd'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4543yakkgjgfb6bv57wksw39ewqd'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw4543tzbg8ekcqtnm9m4xxv30e69k'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw453p3b23ttu6s2h8rcz6hrwu9po4'}\n",
      "1188283809826869248\n",
      "{'pagination_token': '7140dibdnow9c7btw453oz2ifjcaznzl8dbrg83jhcuno'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Query Data from API\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     search_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://api.twitter.com/2/users/\u001b[39m\u001b[39m{\u001b[39;00muserid\u001b[39m}\u001b[39;00m\u001b[39m/liked_tweets\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     output_tweets \u001b[39m=\u001b[39m main(search_url, query_params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTwitter URL: \u001b[39m\u001b[39m{\u001b[39;00msearch_url\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mParams: \u001b[39m\u001b[39m{\u001b[39;00mquery_params\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 21\u001b[0m in \u001b[0;36mmain\u001b[0;34m(url, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(url, params):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     json_response \u001b[39m=\u001b[39m connect_to_endpoint(url, params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# print(json.dumps(json_response, indent=4, sort_keys=True))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m json_response\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 21\u001b[0m in \u001b[0;36mconnect_to_endpoint\u001b[0;34m(url, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect_to_endpoint\u001b[39m(url, params):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, auth\u001b[39m=\u001b[39;49mbearer_oauth, params\u001b[39m=\u001b[39;49mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    524\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    525\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    526\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/requests/sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    648\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/requests/adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 440\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    441\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    442\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    443\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    444\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    445\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    447\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    448\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    449\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    450\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    451\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for batch_of_users, batch_csv in enumerate(dir_list):\n",
    "#     df = pd.read_csv(batch_csv)\n",
    "#     logging.info(f\"Read {batch_csv}\")\n",
    "batch_of_users = 0 # 0.csv\n",
    "tweets_list_for_dataframe = []\n",
    "users_list_for_dataframe = []\n",
    "\n",
    "query_params = {\n",
    "    'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "    'tweet.fields': 'created_at,public_metrics,entities,lang,possibly_sensitive,reply_settings,source,in_reply_to_user_id,geo,referenced_tweets',\n",
    "    'expansions': 'author_id',\n",
    "}\n",
    "\n",
    "hast_next_token = True\n",
    "\n",
    "for userid in list(users_id_list): # There are 200 users id\n",
    "    print(userid)\n",
    "    while hast_next_token:\n",
    "        print('request')\n",
    "        tweets_list_for_dataframe = []\n",
    "        users_list_for_dataframe = []\n",
    "        for attempt in range(N):\n",
    "            print(attempt)\n",
    "            try:\n",
    "                # Query Data from API\n",
    "                query_params['query'] = f'(from:{userid})'\n",
    "                output_tweets = main(search_url, query_params)\n",
    "                logging.info(f'Twitter URL: {search_url}')\n",
    "                logging.info(f'Params: {query_params}')\n",
    "\n",
    "                # If no data found:\n",
    "                if not output_tweets.get('data'):\n",
    "                    logging.warning(f'No data found for userId {userid}')\n",
    "                    # continue\n",
    "                    hast_next_token = False\n",
    "                    break\n",
    "                next_token = {}\n",
    "                next_token['pagination_token'] = output_tweets['meta'].get('next_token')\n",
    "                if next_token['pagination_token']:\n",
    "                    query_params['pagination_token'] = next_token['pagination_token']\n",
    "                    hast_next_token = True\n",
    "                else:\n",
    "                    hast_next_token = False\n",
    "\n",
    "                for tweet in output_tweets['data']:\n",
    "                    # Get new columns\n",
    "                    if not tweet.get('entities'):\n",
    "                        entities_dict = parse_entities({}, {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "                    else:\n",
    "                        entities_dict = parse_entities(tweet['entities'], {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "\n",
    "                    if not tweet.get('public_metrics'):\n",
    "                        public_metrics_dict = expand_dict_object({}, 'public_metrics')\n",
    "                    else:\n",
    "                        public_metrics_dict = expand_dict_object(tweet['public_metrics'], 'public_metrics')\n",
    "\n",
    "                    # Filter out unwanted columns\n",
    "                    tweet = {key: tweet[key] for key in tweet.keys() if key in json_tweets_columns}\n",
    "\n",
    "                    # Combine dicts\n",
    "                    tweet = {**tweet, **entities_dict, **public_metrics_dict, **next_token, 'current_time': datetime.now()}\n",
    "                    # tweet = sorted(tweet.items(), key=lambda kv: kv[1])\n",
    "                    tweets_list_for_dataframe.append(tweet)\n",
    "\n",
    "# for userid in list(users_id_list): # There are 200 users id\n",
    "#     while hast_next_token:\n",
    "#         for attempt in range(N):\n",
    "#             try:\n",
    "#                 # Query Data from API\n",
    "#                 search_url = f\"https://api.twitter.com/2/users/{userid}/liked_tweets\"\n",
    "#                 output_tweets = main(search_url, query_params)\n",
    "#                 logging.info(f'Twitter URL: {search_url}')\n",
    "#                 logging.info(f'Params: {query_params}')\n",
    "\n",
    "#                 # If no data found:\n",
    "#                 if not output_tweets.get('data'):\n",
    "#                     logging.warning(f'No data found for userId {userid}')\n",
    "#                     continue\n",
    "\n",
    "#                 next_token = {}\n",
    "#                 next_token['pagination_token'] = output_tweets['meta'].get('next_token')\n",
    "#                 if next_token['pagination_token']:\n",
    "#                     query_params['pagination_token'] = next_token['pagination_token']\n",
    "#                     hast_next_token = True\n",
    "#                 else:\n",
    "#                     hast_next_token = False\n",
    "\n",
    "#                 for tweet in output_tweets['data']:\n",
    "#                     # Get new columns\n",
    "#                     if not tweet.get('entities'):\n",
    "#                         entities_dict = parse_entities({}, {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "#                     else:\n",
    "#                         entities_dict = parse_entities(tweet['entities'], {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "                    \n",
    "#                     if not tweet.get('public_metrics'):\n",
    "#                         public_metrics_dict = expand_dict_object({}, 'public_metrics')\n",
    "#                     else:\n",
    "#                         public_metrics_dict = expand_dict_object(tweet['public_metrics'], 'public_metrics')\n",
    "\n",
    "#                     # Filter out unwanted columns\n",
    "#                     tweet = {key: tweet[key] for key in tweet.keys() if key in json_tweets_columns}\n",
    "\n",
    "#                     # Combine dicts\n",
    "#                     tweet = {**tweet, **entities_dict, **public_metrics_dict, **next_token, 'current_time': datetime.now()}\n",
    "#                     tweets_list_for_dataframe.append(tweet)\n",
    "\n",
    "            except Exception as err:\n",
    "                if err.args[0] == 429: # If ERROR = 429 (Too Many Requests, wait for retry) \n",
    "                    logging.info(f'Error 429. Sleep: {sleep}')\n",
    "                    time.sleep(sleep)\n",
    "                    # continue\n",
    "                    raise\n",
    "                else:\n",
    "                    raise\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().minute/2 < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if datetime.now().minute/2 < 20 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/7czjs3z54yd54jz8j2ktmygc0000gn/T/ipykernel_6004/3436623643.py:114: DtypeWarning: Columns (6,7,8,9,10,11,12,13,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"{path}/{batch_csv}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188283809826869248\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "914779140225675265\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n",
      "True\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m# query_params['query'] = f'(from:{userid})'\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m output_tweets \u001b[39m=\u001b[39m main(search_url, query_params)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTwitter URL: \u001b[39m\u001b[39m{\u001b[39;00msearch_url\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 23\u001b[0m in \u001b[0;36mmain\u001b[0;34m(url, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(url, params):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     json_response \u001b[39m=\u001b[39m connect_to_endpoint(url, params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# print(json.dumps(json_response, indent=4, sort_keys=True))\u001b[39;00m\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 23\u001b[0m in \u001b[0;36mconnect_to_endpoint\u001b[0;34m(url, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n",
      "\u001b[0;31mException\u001b[0m: (429, '{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError 429. Sleep: \u001b[39m\u001b[39m{\u001b[39;00msleep\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m     hast_next_token \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(sleep)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/liked_tweets.ipynb#X32sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# logging.basicConfig(filename='App_LikedTweets_ServerStatus.log', encoding='utf-8', level=logging.ERROR, force=True)\n",
    "\n",
    "# Load Credentials\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "BEARER_TOKEN_2 = config['BEARER_TOKEN_2']\n",
    "BEARER_TOKEN_3 = config['BEARER_TOKEN_3']\n",
    "\n",
    "BEARER_TOKEN = BEARER_TOKEN_2 if datetime.now().minute/2 < 20 else BEARER_TOKEN_3\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def main(url, params):\n",
    "    json_response = connect_to_endpoint(url, params)\n",
    "    # print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    return json_response\n",
    "\n",
    "def get_list_of_items(list_of_dict: list, key_name: str) -> list:\n",
    "    if not list_of_dict: # If list_of_dict is None\n",
    "        return []\n",
    "    return [entity[key_name] for entity in list_of_dict]\n",
    "\n",
    "def parse_entities(tweet_entities: dict, object_dict: dict, prefix: str = '') -> dict:\n",
    "    entities_dict = defaultdict(list)\n",
    "\n",
    "    # Retrieve Entities Objects\n",
    "    for object_name, key_name in object_dict.items():\n",
    "        column_name = f\"{prefix}{object_name}_list\"\n",
    "        # print(object_name)\n",
    "        entities_dict[column_name] = get_list_of_items(tweet_entities.get(object_name), key_name)\n",
    "    return entities_dict\n",
    "    # entities_dict['hashtags_list'] = get_list_of_items(tweet_entities['hashtags'], 'tag')\n",
    "    # entities_dict['urls_list'] = get_list_of_items(tweet_entities['urls'], 'expanded_url')\n",
    "\n",
    "def expand_dict_object(object_dict: dict, column_prefix: str,key_names_list: list = []) -> dict:\n",
    "    \"\"\"Return key,value pairs from dict. Returns selected keys in key_names_list or all if key_names_list is []\n",
    "\n",
    "    Args:\n",
    "        object_dict (dict)\n",
    "        key_names_list (list)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    result_dict = defaultdict(list)\n",
    "    for key_name, value_name in object_dict.items():\n",
    "        column_name = f\"{column_prefix}_{key_name}\"\n",
    "        if not key_names_list:\n",
    "            result_dict[column_name] = value_name\n",
    "        elif key_name in key_names_list:\n",
    "            result_dict[column_name] = value_name\n",
    "    return result_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    json_tweets_columns = [\n",
    "        'id', 'author_id', 'possibly_sensitive', 'edit_history_tweet_ids', 'lang',\n",
    "        'source', 'reply_settings', 'text', 'created_at'\n",
    "    ]\n",
    "\n",
    "    json_users_columns = [\n",
    "        'id', 'name', 'username', 'location', 'url', 'created_at', 'username',\n",
    "        'profile_image_url', 'profile_image_url', 'verified', 'description',\n",
    "        'protected'\n",
    "    ]\n",
    "    user_columns = [\n",
    "        'profile_image_url', 'username', 'protected', 'name', 'id', 'description', \n",
    "        'created_at', 'verified', 'location', 'url_urls_list', 'description_hashtags_list', 'description_urls_list', 'description_mentions_list',\n",
    "        'description_cashtags_list', 'public_metrics_followers_count', 'public_metrics_following_count', 'public_metrics_tweet_count', \n",
    "        'public_metrics_listed_count', 'current_time'\n",
    "    ]\n",
    "\n",
    "    tweet_columns = [\n",
    "        'possibly_sensitive', 'text', 'source', 'id', 'created_at', 'lang', 'reply_settings', 'author_id', 'edit_history_tweet_ids', \n",
    "        'hashtags_list', 'urls_list', 'public_metrics_retweet_count', 'public_metrics_reply_count', 'public_metrics_like_count', \n",
    "        'public_metrics_quote_count', 'pagination_token', 'current_time'\n",
    "    ]\n",
    "    # Compute Start and End Time with 6 month interval\n",
    "\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    date_str = f'{current_time.year}-{current_time.month:02d}-{current_time.day:02d}'\n",
    "    time_str = f'{current_time.hour:02d}-{current_time.minute:02d}'\n",
    "\n",
    "    sleep = 300 # in seconds\n",
    "    N = 5 # Number of retries. Maxium retry time = N*sleep (s)\n",
    "\n",
    "    # path = '/project/ll_774_951/SMHabits/avax_project_data/random_user_tweets'\n",
    "    path = 'avax_data'\n",
    "    dir_list = os.listdir(path)\n",
    "\n",
    "    for batch_of_users, batch_csv in enumerate(dir_list):\n",
    "        df = pd.read_csv(f\"{path}/{batch_csv}\")\n",
    "        logging.info(f\"Read {batch_csv}\")\n",
    "\n",
    "        # Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "        # expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "        query_params = {\n",
    "            'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "            'tweet.fields': 'created_at,public_metrics,entities,lang,possibly_sensitive,reply_settings,source,in_reply_to_user_id,geo,referenced_tweets',\n",
    "            'expansions': 'author_id',\n",
    "        }\n",
    "\n",
    "        hast_next_token = True\n",
    "        users_id_list = set(df['userid'])\n",
    "        # Create CSV file\n",
    "        if not os.path.exists(f\"data/liked-tweets-{batch_of_users}.csv\"):\n",
    "            with open(f\"data/liked-tweets-{batch_of_users}.csv\", \"a+\", encoding='utf-8') as f:\n",
    "                f.write(\", \".join(tweet_columns) + \"\\n\")\n",
    "        for userid in list(users_id_list): # There are 200 users id\n",
    "            print(userid)\n",
    "            hast_next_token = True\n",
    "            while hast_next_token:\n",
    "                print(hast_next_token)\n",
    "                tweets_list_for_dataframe = []\n",
    "                users_list_for_dataframe = []\n",
    "                for attempt in range(N):\n",
    "                    print(attempt)\n",
    "                    try:\n",
    "                        # Query Data from API\n",
    "                        search_url = f\"https://api.twitter.com/2/users/{userid}/liked_tweets\"\n",
    "\n",
    "                        # If user data has already been collected, skip\n",
    "                        try:\n",
    "                            save_csv = True\n",
    "                            tweets_history = set(pd.read_csv('data/liked-tweets-0.csv', usecols=[' id'])[' id'].values)\n",
    "                        except: # if file doesn't exist\n",
    "                            tweets_history = {}\n",
    "\n",
    "                        # query_params['query'] = f'(from:{userid})'\n",
    "                        output_tweets = main(search_url, query_params)\n",
    "                        logging.info(f'Twitter URL: {search_url}')\n",
    "                        logging.info(f'Params: {query_params}')\n",
    "\n",
    "                        # If no data found:\n",
    "                        if not output_tweets.get('data'):\n",
    "                            save_csv = False\n",
    "                            logging.warning(f'No data found for userId {userid}')\n",
    "                            # continue\n",
    "                            hast_next_token = False\n",
    "                            break\n",
    "                        next_token = {}\n",
    "                        next_token['pagination_token'] = output_tweets['meta'].get('next_token')\n",
    "                        if next_token['pagination_token']:\n",
    "                            query_params['pagination_token'] = next_token['pagination_token']\n",
    "                            hast_next_token = True\n",
    "                        else:\n",
    "                            hast_next_token = False\n",
    "\n",
    "                        for tweet in output_tweets['data']:\n",
    "                            \n",
    "                            # Check if tweet id has already been collected\n",
    "                            if tweet['id'] in tweets_history:\n",
    "                                logging.warning(f'Reapeated tweet_id for {userid}: stopping historical data collection')\n",
    "                                print(f'Reapeated tweet_id for {userid}: stopping historical data collection')\n",
    "                                hast_next_token = False # break from while loop\n",
    "                                break\n",
    "\n",
    "                            # Get new columns\n",
    "                            if not tweet.get('entities'):\n",
    "                                entities_dict = parse_entities({}, {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "                            else:\n",
    "                                entities_dict = parse_entities(tweet['entities'], {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "\n",
    "                            if not tweet.get('public_metrics'):\n",
    "                                public_metrics_dict = expand_dict_object({}, 'public_metrics')\n",
    "                            else:\n",
    "                                public_metrics_dict = expand_dict_object(tweet['public_metrics'], 'public_metrics')\n",
    "\n",
    "                            # Filter out unwanted columns\n",
    "                            tweet = {key: tweet[key] for key in tweet.keys() if key in json_tweets_columns}\n",
    "\n",
    "                            # Combine dicts\n",
    "                            tweet = {**tweet, **entities_dict, **public_metrics_dict, **next_token, 'current_time': datetime.now()}\n",
    "                            # tweet = sorted(tweet.items(), key=lambda kv: kv[1])\n",
    "                            tweets_list_for_dataframe.append(tweet)\n",
    "\n",
    "                    except Exception as err:\n",
    "                        if err.args[0] == 429: # If ERROR = 429 (Too Many Requests, wait for retry) \n",
    "                            logging.info(f'Error 429. Sleep: {sleep}')\n",
    "                            hast_next_token = False\n",
    "                            time.sleep(sleep)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                    break\n",
    "                if save_csv:\n",
    "                    tweets =  pd.DataFrame.from_records(tweets_list_for_dataframe, columns=tweet_columns)\n",
    "                    tweets.to_csv(f'data/liked-tweets-{batch_of_users}.csv', mode='a', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_history = set(pd.read_csv('data/liked-tweets-0.csv', usecols=[' id'])[' id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if 1436866917508136960 in tweets_history else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets =  pd.DataFrame.from_records(tweets_list_for_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_stream/users-stream.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['id', 'verified']].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    10860\n",
       "True      3774\n",
       "Name: verified, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['verified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25789257892578926"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3774/(10860+3774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(f'data/liked-tweets-{batch_of_users}-{date_str}-{time_str}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed29fcc266c631214eb0a32dcb461d51a7279cf73c87ecfbb65b364d13f4dbad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
