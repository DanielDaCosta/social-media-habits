{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='App_Stream_ServerStatus.log', encoding='utf-8', level=logging.ERROR, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Crentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "CONSUMER_KEY = config['CONSUMER_KEY']\n",
    "CONSUMER_SECRET = config['CONSUMER_SECRET']\n",
    "BEARER_TOKEN = config['BEARER_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class Stream(tweepy.Stream):\n",
    "    def on_status(self, status):\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "\n",
    "        with open(\"out.csv\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(\"%s,%s,%s,%s,%s,%s\\n\" % (status.created_at,status.user.screen_name,is_retweet,is_quote,text,quoted_text))\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(data=[StreamRule(value='Tweepy', tag=None, id='1588313169058877440')], includes={}, errors=[], meta={'sent': '2022-11-03T23:32:23.632Z', 'summary': {'created': 1, 'not_created': 0, 'valid': 1, 'invalid': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_client = tweepy.StreamingClient(BEARER_TOKEN)\n",
    "streaming_client.add_rules(tweepy.StreamRule(\"Tweepy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_items(list_of_dict: list, key_name: str) -> list:\n",
    "    if not list_of_dict: # If list_of_dict is None\n",
    "        return []\n",
    "    return [entity[key_name] for entity in list_of_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities(tweet_entities: dict, object_dict: dict, prefix: str = '') -> dict:\n",
    "    entities_dict = defaultdict(list)\n",
    "\n",
    "    # Retrieve Entities Objects\n",
    "    for object_name, key_name in object_dict.items():\n",
    "        column_name = f\"{prefix}{object_name}_list\"\n",
    "        # print(object_name)\n",
    "        entities_dict[column_name] = get_list_of_items(tweet_entities.get(object_name), key_name)\n",
    "    return entities_dict\n",
    "    # entities_dict['hashtags_list'] = get_list_of_items(tweet_entities['hashtags'], 'tag')\n",
    "    # entities_dict['urls_list'] = get_list_of_items(tweet_entities['urls'], 'expanded_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dict_object(object_dict: dict, column_prefix: str,key_names_list: list = []) -> dict:\n",
    "    \"\"\"Return key,value pairs from dict. Returns selected keys in key_names_list or all if key_names_list is []\n",
    "\n",
    "    Args:\n",
    "        object_dict (dict)\n",
    "        key_names_list (list)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    result_dict = defaultdict(list)\n",
    "    for key_name, value_name in object_dict.items():\n",
    "        column_name = f\"{column_prefix}_{key_name}\"\n",
    "        if not key_names_list:\n",
    "            result_dict[column_name] = value_name\n",
    "        elif key_name in key_names_list:\n",
    "            result_dict[column_name] = value_name\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tweets_columns = [\n",
    "    'id', 'author_id', 'possibly_sensitive', 'edit_history_tweet_ids', 'lang',\n",
    "    'source', 'reply_settings', 'text', 'created_at'\n",
    "]\n",
    "\n",
    "json_users_columns = [\n",
    "    'id', 'name', 'username', 'location', 'url', 'created_at', 'username',\n",
    "    'profile_image_url', 'profile_image_url', 'verified', 'description',\n",
    "    'protected'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingClient(tweepy.StreamingClient):\n",
    "\n",
    "    # def on_tweet(self, tweet):\n",
    "    #     # print(status.id_str)\n",
    "    #     # # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "    #     # is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "    #     # # check if text has been truncated\n",
    "    #     # if hasattr(status,\"extended_tweet\"):\n",
    "    #     #     text = status.extended_tweet[\"full_text\"]\n",
    "    #     # else:\n",
    "    #     #     text = status.text\n",
    "\n",
    "    #     # # check if this is a quote tweet.\n",
    "    #     # is_quote = hasattr(status, \"quoted_status\")\n",
    "    #     # quoted_text = \"\"\n",
    "    #     # if is_quote:\n",
    "    #     #     # check if quoted tweet's text has been truncated before recording it\n",
    "    #     #     if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "    #     #         quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "    #     #     else:\n",
    "    #     #         quoted_text = status.quoted_status.text\n",
    "\n",
    "    #     # # remove characters that might cause problems with csv encoding\n",
    "    #     # remove_characters = [\",\",\"\\n\"]\n",
    "    #     # for c in remove_characters:\n",
    "    #     #     text.replace(c,\" \")\n",
    "    #     #     quoted_text.replace(c, \" \")\n",
    "    #     print(tweet)\n",
    "    #     with open(\"out.csv\", \"a\", encoding='utf-8') as f:\n",
    "    #         f.write(\"%s\\n\" % (tweet.id))\n",
    "    def on_data(self ,raw_data):\n",
    "        tweets_list_for_dataframe = []\n",
    "        users_list_for_dataframe = []\n",
    "        output_tweets = json.loads(raw_data)\n",
    "                        # If no data found:\n",
    "        if not output_tweets.get('data'):\n",
    "            # logging.warning(f'No data found for userId {userid}')\n",
    "            return\n",
    "        # next_token = {}\n",
    "        # next_token['next_token'] = output_tweets['meta'].get('next_token')\n",
    "        # if next_token['pagination_token']:\n",
    "        #     query_params['pagination_token'] = next_token['pagination_token']\n",
    "        #     hast_next_token = True\n",
    "        # else:\n",
    "        #     hast_next_token = False\n",
    "\n",
    "        for tweet in output_tweets['data']:\n",
    "            # Get new columns\n",
    "            if not tweet.get('entities'):\n",
    "                entities_dict = parse_entities({}, {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "            else:\n",
    "                entities_dict = parse_entities(tweet['entities'], {'hashtags': 'tag', 'urls': 'expanded_url'})\n",
    "            \n",
    "            if not tweet.get('public_metrics'):\n",
    "                public_metrics_dict = expand_dict_object({}, 'public_metrics')\n",
    "            else:\n",
    "                public_metrics_dict = expand_dict_object(tweet['public_metrics'], 'public_metrics')\n",
    "\n",
    "            # Filter out unwanted columns\n",
    "            tweet = {key: tweet[key] for key in tweet.keys() if key in json_tweets_columns}\n",
    "\n",
    "            # Combine dicts\n",
    "            tweet = {**tweet, **entities_dict, **public_metrics_dict, 'current_time': datetime.now()}\n",
    "            tweets_list_for_dataframe.append(tweet)\n",
    "\n",
    "        # for user in output_tweets['includes']['users']:\n",
    "\n",
    "        #     # Get new columns\n",
    "        #     if (not user.get('entities')) or (not user.get('entities').get('url')):\n",
    "        #         url_dict = parse_entities({}, {'urls': 'expanded_url'}, prefix='url_')\n",
    "        #     else:\n",
    "        #         url_dict = parse_entities(user['entities']['url'], {'urls': 'expanded_url'}, prefix='url_')\n",
    "\n",
    "        #     if (not user.get('entities')) or (not user.get('entities').get('description')):\n",
    "        #         description_dict = parse_entities(\n",
    "        #             {},\n",
    "        #             {'hashtags': 'tag', 'urls': 'expanded_url', 'mentions': 'username', 'cashtags': 'tag'}, prefix='description_')\n",
    "        #     else:\n",
    "        #         description_dict = parse_entities(\n",
    "        #             user['entities']['description'],\n",
    "        #             {'hashtags': 'tag', 'urls': 'expanded_url', 'mentions': 'username', 'cashtags': 'tag'}, prefix='description_')\n",
    "\n",
    "        #     user_public_metrics_dict = expand_dict_object(user['public_metrics'], 'public_metrics')\n",
    "        #     # Filter out unwanted columns\n",
    "        #     user = {key: user[key] for key in user.keys() if key in json_users_columns}\n",
    "\n",
    "        #     # Combine dicts\n",
    "        #     user = {**user, **url_dict, **description_dict, **user_public_metrics_dict, 'current_time': datetime.now()}\n",
    "        #     users_list_for_dataframe.append(user)\n",
    "        print(tweets_list_for_dataframe)\n",
    "    # def on_includes(self ,includes):\n",
    "    #     pass\n",
    "\n",
    "    # def on_error(self, status_code):\n",
    "    #     print(\"Encountered streaming error (\", status_code, \")\")\n",
    "    #     sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_client.delete_rules(['1588313169058877440', '1578971194384011265', '1578971194384011264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_params = {\n",
    "    # 'query': '(from:twitterdev) has:links has:hashtags lang:en',\n",
    "    'user_fields': ['created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'],\n",
    "    'tweet.fields': 'created_at,public_metrics,entities,lang,possibly_sensitive,reply_settings,source,in_reply_to_user_id,geo',\n",
    "    'expansions': 'author_id',\n",
    "    'start_time': START_TIME,\n",
    "    'end_time': END_TIME,\n",
    "    'max_results': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list_for_dataframe = []\n",
    "users_list_for_dataframe = []\n",
    "streaming_client = StreamingClient(BEARER_TOKEN)\n",
    "# streaming_client.add_rules(tweepy.StreamRule(\"Tweepy\"))\n",
    "with open(\"out.csv\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "# tags = [\"hate speech\"]\n",
    "streaming_client.sample(expansions='author_id', tweet_fields='created_at,public_metrics,entities,lang,possibly_sensitive,reply_settings,source,in_reply_to_user_id,geo', user_fields='created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'consumer_key', 'consumer_secret', 'access_token', and 'access_token_secret'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/stream_search.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/stream_search.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m api \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39mAPI(auth)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/stream_search.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# initialize stream\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/stream_search.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m streamListener \u001b[39m=\u001b[39m Stream()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/stream_search.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m stream \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39mStream(auth\u001b[39m=\u001b[39mapi\u001b[39m.\u001b[39mauth, listener\u001b[39m=\u001b[39mstreamListener,tweet_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mextended\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/SM-Habits/social-media-habits/stream_search.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mout.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'consumer_key', 'consumer_secret', 'access_token', and 'access_token_secret'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # complete authorization and initialize API endpoint\n",
    "    # auth = tweepy.OAuth2BearerHandler(BEARER_TOKEN)\n",
    "    auth = tweepy.Client(BEARER_TOKEN)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # initialize stream\n",
    "    streamListener = Stream()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "    with open(\"out.csv\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"date,user,is_retweet,is_quote,text,quoted_text\\n\")\n",
    "    tags = [\"hate speech\"]\n",
    "    stream.filter(track=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed29fcc266c631214eb0a32dcb461d51a7279cf73c87ecfbb65b364d13f4dbad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
